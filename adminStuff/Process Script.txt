In simple terms, we need to find a puzzle within an image, reconize the digits within the puzzle, solve the puzzle, and then display the solved solution.

For the first step, we need to find the puzzle within an image. So our first step is to process an image (similar to edge detection). With out processed image, we can find the 'contours' (openCV term for enclosed shape within image) and sort them by size. We assume the largest enclosed shape within the image is our puzzle. At this point we need to calculate the perspective transform that tranforms the potentially skewed puzzle into a square image, and transform the image. At the same time, we cache the perspective transform that undoes this transformation for later.

Now that we have found the puzzle within an image, we need to recognize the provided digits within the image. Firstly, we need to process each individual digit square and prep it for OCR. This is done with a simple blur and thresholding to reduce noise. Now we use tesseract (which is an open source machine learning platform) along with our custom training data, to detect what each digit is. Among high resolution images using the same font we trained with, we can almost always find the digits provided. The only circumstances we fail to find the digits is when the puzzle image plane is massively skewed compared to the camera. 

Originally, we planned to solve puzzles and display the results in Augmented Reality (in other words, display the results over a live video stream). The only problem is with the limited amount of training data we could put together, we need a very high resolution in order to detect images (the images we use are around 4k). Our webcams on our laptops were far from 4k quality and we couldn't continue testing. Our project has been built with video in mind, and should work when processing video. We accomplish this by waiting until we find a puzzle within the frame, capture the puzzle, OCR the results (which takes some time), solve the puzzle (which also takes time), and then cache the results to display each frame over the live video.

Now we need to solve the puzzle. Since Sudoku is obviously an NP problem, and studying NP problems is not the point of this class, we instead opted to use a public library to solve our sudoku solutions. It is not particulary fast, but since we had to cut live video solving, this library is good enough.

Finally, we print our results back onto the image. We use native openCV functions to print text for each solved digit on top of the puzzle image. Then, using the perspective transform we cached earlier, we can un-transform the puzzle. Once we have an un-transformed puzzle, we overlay it back onto the original image. And thus, the objective is complete.